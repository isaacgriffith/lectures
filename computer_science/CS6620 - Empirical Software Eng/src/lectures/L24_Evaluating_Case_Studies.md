---
author:
- Isaac Griffith
title: Evaluating Case Studies
institute: |
  CS 6620

  Department of Informatics and Computer Science

  Idaho State University
fontsize: 12pt
cornerLogo: "images/spirit.png"
wideLogo: "images/wide.png"
lowerCornerLogo: "images/roar.eps"
...

# Introduction

5 Things should be considered when reviewing/reading a case study:

* Case Study Design
* Data Collection Methods
* Data Analysis and Interpretation
* Reporting and Dissemination
* Overall Review

# Case Study Design

1. What is the case and its units of analysis?
2. Are clear objectives, preliminary research questions hypotheses (if any) defined in advance?
3. Is the theoretical basis---relation to existing literature or other cases---defined?
4. Are the authors? intentions with the research made clear?
5. Is the case adequately defined (size, domain, process, subjects?)?

# Case Study Design

6. Is a cause-effect relation under study? If yes, is it possible to distinguish the cause from other factors using the proposed design?
7. Does the design involve data from multiple sources (data triangulation), using multiple methods (method triangulation)?
8. Is there a rationale behind the selection of subjects, roles, artifacts, viewpoints, and so on?
9. Is the specified case relevant to validly address the research questions (construct validity)?
10. Is the integrity of individuals/organizations taken into account?

# Data Collection

11. Is a case study protocol for data collection and analysis derived (what, why, how, when)? Are procedures for its update defined?
12. Are multiple data sources and collection methods planned (triangulation)?
13. Are measurement instruments and procedures well defined (measurement definitions, interview questions)?
14. Are the planned methods and measurements sufficient to fulfill the objective of the study?
15. Is the study design approved by a review board, and has informed consent been obtained from individuals and organizations?
16. Is data collected according to the case study protocol?

# Data Collection

17. Is the observed phenomenon correctly implemented (e.g., to what extent is a design method under study actually used)?
18. Is data recorded to enable further analysis?
19. Are sensitive results identified (for individuals, the organization or the project)?
20. Are the data collection procedures well traceable?
21. Does the collected data provide ability to address the research question?

# Data Analysis and Interpretation

22. Is the analysis methodology defined, including roles and review procedures?
23. Is a chain of evidence shown with traceable inferences from data to research questions and existing theory?
24. Are alternative perspectives and explanations used in the analysis?
25. Is a cause-effect relation under study? If yes, is it possible to distinguish the cause from other factors in the analysis?
26. Are there clear conclusions from the analysis, including recommendations for practice/further research?
27. Are threats to the validity analyzed in a systematic way and countermeasures taken? (Construct, internal, external, reliability)

# Reporting and Dissemination

28. Are the case and its units of analysis adequately presented?
29. Are the objective, the research questions and corresponding answers reported?
30. Are related theory and hypotheses clearly reported?
31. Are the data collection procedures presented, with relevant motivation?
32. Is sufficient raw data presented (e.g., real-life examples, quotations)?
33. Are the analysis procedures clearly reported?

# Reporting and Dissemination

34. Are threats to validity analyses reported along with countermeasures taken to reduce threats?
35. Are ethical issues reported openly (personal intentions, integrity issues, confidentiality)
36. Does the report contain conclusions, implications for practice, and future research?
37. Does the report give a realistic and credible impression?
38. Is the report suitable for its audience, easy to read, and well structured?

# Overall Checks

39. Are the objective, research questions, and hypotheses (if applicable) clear and relevant? (1, 2, 5, 29, 30)
40. Are the case and its units of analysis well defined? (1, 5, 28)
41. Is the suitability of the case to address the research questions clearly motivated? (8, 9, 14)
42. Is the case study based on theory or linked to existing literature? (3)
43. Are the data collection procedures sufficient for the purpose of the case study (data sources, collection, validation)? (11, 13, 16, 18, 21, 31)
44. Is sufficient raw data presented to provide understanding of the case and the analysis? (32)

# Overall Checks

45. Are the analysis procedures sufficient for the purpose of the case study (repeatable, transparent)? (22, 33)
46. Is a clear chain of evidence established from observations to conclusions? (6, 17, 20, 23, 25)
47. Are threats to validity analyses conducted in a systematic way and are countermeasures taken to reduce threats? (27, 34, 37)
48. Is triangulation applied (multiple collection and analysis methods, multiple authors, multiple theories)? (7, 12, 22, 24)
49. Are ethical issues properly addressed (personal intentions, integrity, confidentiality, consent, review board approval)? (4, 10, 15, 19, 35)
50. Are conclusions, implications for practice and future research, suitably reported for its audience? (26, 29, 36, 37, 38)

#

\centering
\includegraphics[scale=.40]{images/questions.png}

\Huge \textbf{Are there any questions?}
